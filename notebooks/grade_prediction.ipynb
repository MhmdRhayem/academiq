{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Grade Prediction\n",
    "\n",
    "Train on S1-S4 course grades â†’ Predict S5-S6 course grades\n",
    "\n",
    "With hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('cleaned_data.xlsx')\n",
    "grade_matrix = df.pivot_table(index='admi', columns='code', values='note', aggfunc='first')\n",
    "print(f\"Grade matrix: {grade_matrix.shape[0]} students x {grade_matrix.shape[1]} courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get courses by semester\n",
    "courses_s1_s4 = []\n",
    "courses_s5_s6 = []\n",
    "\n",
    "for sem in ['S1', 'S2', 'S3', 'S4']:\n",
    "    courses_s1_s4.extend(df[df['simester'] == sem]['code'].unique().tolist())\n",
    "for sem in ['S5', 'S6']:\n",
    "    courses_s5_s6.extend(df[df['simester'] == sem]['code'].unique().tolist())\n",
    "\n",
    "courses_s1_s4 = list(dict.fromkeys([c for c in courses_s1_s4 if c in grade_matrix.columns]))\n",
    "courses_s5_s6 = list(dict.fromkeys([c for c in courses_s5_s6 if c in grade_matrix.columns]))\n",
    "\n",
    "print(f\"Input (S1-S4): {len(courses_s1_s4)} courses\")\n",
    "print(f\"Output (S5-S6): {len(courses_s5_s6)} courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = grade_matrix[courses_s1_s4].fillna(grade_matrix[courses_s1_s4].median())\n",
    "y = grade_matrix[courses_s5_s6].fillna(grade_matrix[courses_s5_s6].median())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"Random Forest Parameter Grid:\")\n",
    "for param, values in rf_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in rf_param_grid.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning Random Forest (this may take a few minutes)...\")\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest RF Parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best CV R2 Score: {rf_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best RF on test set\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Random Forest Test Results:\")\n",
    "print(f\"  RMSE: {rf_rmse:.2f}\")\n",
    "print(f\"  R2: {rf_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameter grid\n",
    "xgb_param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 200],\n",
    "    'estimator__max_depth': [3, 5, 7, 10],\n",
    "    'estimator__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'estimator__subsample': [0.7, 0.8, 1.0],\n",
    "    'estimator__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"XGBoost Parameter Grid:\")\n",
    "for param, values in xgb_param_grid.items():\n",
    "    print(f\"  {param.replace('estimator__', '')}: {values}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in xgb_param_grid.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning XGBoost (this may take several minutes)...\")\n",
    "\n",
    "# Use RandomizedSearchCV for faster tuning due to many combinations\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "xgb_base = MultiOutputRegressor(xgb.XGBRegressor(random_state=42, n_jobs=1))\n",
    "\n",
    "xgb_grid = RandomizedSearchCV(\n",
    "    xgb_base,\n",
    "    xgb_param_grid,\n",
    "    n_iter=50,  # Test 50 random combinations\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest XGB Parameters:\")\n",
    "for param, value in xgb_grid.best_params_.items():\n",
    "    print(f\"  {param.replace('estimator__', '')}: {value}\")\n",
    "print(f\"Best CV R2 Score: {xgb_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best XGB on test set\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "xgb_pred = xgb_best.predict(X_test)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "print(f\"XGBoost Test Results:\")\n",
    "print(f\"  RMSE: {xgb_rmse:.2f}\")\n",
    "print(f\"  R2: {xgb_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'RMSE':<10} {'R2':<10} {'CV R2':<10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Random Forest':<20} {rf_rmse:<10.2f} {rf_r2:<10.4f} {rf_grid.best_score_:<10.4f}\")\n",
    "print(f\"{'XGBoost':<20} {xgb_rmse:<10.2f} {xgb_r2:<10.4f} {xgb_grid.best_score_:<10.4f}\")\n",
    "\n",
    "if rf_r2 >= xgb_r2:\n",
    "    best_model, best_name, best_params = rf_best, 'RandomForest', rf_grid.best_params_\n",
    "else:\n",
    "    best_model, best_name, best_params = xgb_best, 'XGBoost', xgb_grid.best_params_\n",
    "\n",
    "print(f\"\\nBest Model: {best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Models with Hyperparameter Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Create descriptive filenames with hyperparameters\n",
    "rf_params = rf_grid.best_params_\n",
    "rf_filename = f\"rf_n{rf_params['n_estimators']}_d{rf_params['max_depth']}_split{rf_params['min_samples_split']}_leaf{rf_params['min_samples_leaf']}.pkl\"\n",
    "\n",
    "xgb_params = {k.replace('estimator__', ''): v for k, v in xgb_grid.best_params_.items()}\n",
    "xgb_filename = f\"xgb_n{xgb_params['n_estimators']}_d{xgb_params['max_depth']}_lr{xgb_params['learning_rate']}_sub{xgb_params['subsample']}_col{xgb_params['colsample_bytree']}.pkl\"\n",
    "\n",
    "# Save models\n",
    "joblib.dump(rf_best, f'models/{rf_filename}')\n",
    "joblib.dump(xgb_best, f'models/{xgb_filename}')\n",
    "joblib.dump(best_model, 'models/best_model.pkl')\n",
    "joblib.dump(courses_s1_s4, 'models/feature_columns.pkl')\n",
    "joblib.dump(courses_s5_s6, 'models/target_columns.pkl')\n",
    "\n",
    "print(\"Models saved:\")\n",
    "print(f\"  - {rf_filename}\")\n",
    "print(f\"  - {xgb_filename}\")\n",
    "print(f\"  - best_model.pkl ({best_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'best_model': best_name,\n",
    "    'random_forest': {\n",
    "        'params': rf_params,\n",
    "        'filename': rf_filename,\n",
    "        'rmse': float(rf_rmse),\n",
    "        'r2': float(rf_r2),\n",
    "        'cv_r2': float(rf_grid.best_score_)\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'params': xgb_params,\n",
    "        'filename': xgb_filename,\n",
    "        'rmse': float(xgb_rmse),\n",
    "        'r2': float(xgb_r2),\n",
    "        'cv_r2': float(xgb_grid.best_score_)\n",
    "    },\n",
    "    'input_courses': courses_s1_s4,\n",
    "    'output_courses': courses_s5_s6\n",
    "}\n",
    "\n",
    "with open('models/model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nMetadata saved to models/model_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved files\n",
    "print(\"\\nAll files in models/:\")\n",
    "for f in sorted(os.listdir('models')):\n",
    "    size = os.path.getsize(f'models/{f}') / 1024\n",
    "    print(f\"  - {f} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mjoblib\u001b[49m.load(\u001b[33m'\u001b[39m\u001b[33mmodels/best_model.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m input_courses = joblib.load(\u001b[33m'\u001b[39m\u001b[33mmodels/feature_columns.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m output_courses = joblib.load(\u001b[33m'\u001b[39m\u001b[33mmodels/target_columns.pkl\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model = joblib.load('models/best_model.pkl')\n",
    "input_courses = joblib.load('models/feature_columns.pkl')\n",
    "output_courses = joblib.load('models/target_columns.pkl')\n",
    "\n",
    "# Test student\n",
    "student_id = X_test.index[0]\n",
    "student_input = X_test.loc[[student_id]]\n",
    "student_actual = y_test.loc[student_id]\n",
    "\n",
    "print(f\"Student ID: {student_id}\")\n",
    "print(f\"\\nInput grades (S1-S4):\")\n",
    "for course in input_courses:\n",
    "    print(f\"  {course}: {student_input[course].values[0]:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predicted = model.predict(student_input)[0]\n",
    "\n",
    "print(f\"{'Course':<10} {'Predicted':<12} {'Actual':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for i, course in enumerate(output_courses):\n",
    "    print(f\"{course:<10} {predicted[i]:<12.1f} {student_actual[course]:<10.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_grades(s1_s4_grades):\n",
    "    model = joblib.load('models/best_model.pkl')\n",
    "    input_courses = joblib.load('models/feature_columns.pkl')\n",
    "    output_courses = joblib.load('models/target_columns.pkl')\n",
    "    \n",
    "    X = np.array([[s1_s4_grades.get(c, 50) for c in input_courses]])\n",
    "    predictions = model.predict(X)[0]\n",
    "    \n",
    "    return dict(zip(output_courses, predictions))\n",
    "\n",
    "# Example\n",
    "example = {c: 75 for c in input_courses}\n",
    "result = predict_grades(example)\n",
    "\n",
    "print(\"Example: Student with 75 in all S1-S4 courses\")\n",
    "print(\"\\nPredicted S5-S6 grades:\")\n",
    "for course, grade in result.items():\n",
    "    print(f\"  {course}: {grade:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
